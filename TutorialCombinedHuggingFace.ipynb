{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42e75d6-9625-4cad-975a-efde8bab41fd",
   "metadata": {},
   "source": [
    "# Tutorial on HuggingFace Library:\n",
    "\n",
    "What is HuggingFace. \n",
    "\n",
    "Hugging Face is a company and an open-source community that focuses on natural language processing (NLP) technologies. One of their prominent contributions to the field is the development of Transformers, an open-source library and platform for state-of-the-art natural language processing. The Transformers library is particularly known for its pre-trained models, which include various language models like BERT, GPT (Generative Pre-trained Transformer), and many others.\n",
    "\n",
    "Hugging Face provides a user-friendly interface and tools for working with pre-trained models, making it easier for developers and researchers to implement cutting-edge NLP applications. The library supports a wide range of tasks, such as text classification, named entity recognition, language translation, and more. Hugging Face has gained popularity in the NLP community for its commitment to open-source collaboration, and many researchers and developers contribute to and use their platform to advance the state of the art in natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca179da-c536-4d37-aa2a-32839a7f05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2626a05-8905-4806-aeb6-08c3a128dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450a6f9c2df049eeaadd249012ef185b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffff6953b6a42e6b6714888a28813d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f113c2073074c99939eab63aed747a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7bcceed8974f03a6dd1bbde99f1bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc6476a-8ddb-4ae2-82cf-efa762bf4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b0b7b-152e-49a1-b1d3-c89115925921",
   "metadata": {},
   "source": [
    "# Zero Shot Classification !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9475a-ec4c-46ff-b6e5-0710186c55b5",
   "metadata": {},
   "source": [
    "Zero-shot classification is a type of classification task in natural language processing (NLP) where a model is trained to categorize text samples into classes or categories it has never seen during training. In traditional classification tasks, models are trained on labeled data with examples from each class. However, in zero-shot classification, the model is expected to generalize to new, unseen classes without specific training examples.\n",
    "\n",
    "The term \"zero-shot\" implies that the model is making predictions for classes it has never been explicitly exposed to during training. Instead, the model relies on its ability to understand and generalize patterns from the training data to make predictions on novel classes.\n",
    "\n",
    "This is often achieved by leveraging semantic representations or embeddings of words and sentences. Models pre-trained on large datasets using unsupervised learning, such as BERT (Bidirectional Encoder Representations from Transformers) or GPT (Generative Pre-trained Transformer), have demonstrated strong capabilities in zero-shot learning. These models learn contextualized representations that capture rich semantic information, enabling them to perform well on tasks with limited or no task-specific training data.\n",
    "\n",
    "Zero-shot classification is particularly useful in real-world scenarios where new classes may emerge over time, and it may not be feasible or practical to retrain the model every time new classes are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32ec948-ccb7-4e4c-8599-b0c92c2f02b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0631877527224ea482ad2f34ce5a8fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ec867b27ee4d9a9a71d1c4957df5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02043c9b00dd4cb3906d80233521dd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f1b4c9863c48eabbe4a011dd7d2fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ac7db01f6e425ea9de1a155d7fbfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5129ef6919374c7184b612ca8de476f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445989489555359, 0.11197412759065628, 0.04342695698142052]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cc154-a5f8-40b2-a819-3f366c30bb0a",
   "metadata": {},
   "source": [
    "# Behind the pipeline (PyTorch interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297dde22-a944-490e-aa5d-4d5f71654032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) #tokenizer like in the encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2e7941-f878-4d05-b1bb-11e96f9220f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd6e61c-efc9-45be-8459-ad7b07a40840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8609d98-ba3a-4b9a-bcae-143b46dbd7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb57a6f6-f02e-4005-bd20-df91d1c64e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5fb4b2-eb97-4ab2-aa78-e12305866cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e815e6-4d96-4613-8cc3-ac55fc2b73fb",
   "metadata": {},
   "source": [
    "# LLMs In HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1af8a8-03b4-4f89-8f5d-385b433fd471",
   "metadata": {},
   "source": [
    "In what follows, I will use Mistral 7B. To run on the hardware that\n",
    "I currently have available, I must quantize Mistral to 4Bit, otherwise\n",
    "it would not fit on my GPU. Quantization is a compression technique, \n",
    "how the compression happens is beyond the scope of the course, but it has\n",
    "to deal with information theory. If you have to assign it an idea, in order to\n",
    "remember, it is not very different from clustering the weights. It of course implies\n",
    "some information and quality loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54105bf4-60e1-4d7e-995d-fa1c43857e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1536b502-1de4-4788-a5ab-9ee30c0cb934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a497b0eb072146c9bb4e15fca55e530a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\") #this is just a tokenizer with the ids of the tokens\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6928532c-07aa-4602-bf4b-99091d84167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer = tokenizer, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5608b0-cefd-4620-b092-556c79d6958b",
   "metadata": {},
   "source": [
    "The terms \"top-p\" (nucleus sampling) and \"top-k\" are techniques used in the context of sampling from probability distributions, particularly in natural language processing (NLP) and generative models. These techniques are often employed when generating text or making predictions using models like GPT (Generative Pre-trained Transformer).\n",
    "\n",
    "# Top-k sampling:\n",
    "\n",
    "In top-k sampling, the model generates a set of likely candidates (words or tokens) based on their probabilities.\n",
    "The \"k\" in top-k represents the number of most likely candidates to consider. The model selects from the top-k candidates, effectively narrowing down the cho ces.\n",
    "This helps control the randomness of the generated output and ensures that the model focuses on a smaller set of highly probable options.\n",
    "\n",
    "# Top-p sampling (nucleus sampling):\n",
    "\n",
    "In top-p sampling, the model considers a dynamic set of candidates based on cumulative probabilities.\n",
    "The \"p\" in top-p represents the cumulative probability mass to consider. The model selects from the smallest set of candidates whose cumulative probability exceeds this threshold.\n",
    "This allows for a more flexible approach where the model can include a varying number of candidates based on their probabilities. If some candidates have very low probabilities, they might be excluded.\n",
    "Both top-k and top-p sampling techniques are used to add diversity to the generated output while still maintaining some level of control over the sampling process. They are especially useful in preventing generative models from always producing the same or very similar sequences and can result in more varied and interesting outputs. The choice between top-k and top-p depends on the specific requirements of the task and the desired trade-off between randomness and control in the generated text.erated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f30eb0f3-842d-43a7-9216-fb8569201632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sbr/anaconda3/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a data scientist, can you explain the concept of regularization in machine learning?\n",
      "\n",
      "Regularization is a technique used in machine learning to prevent overfitting of models. Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new, unseen data. Regularization adds a penalty term to the loss function of the model, which discourages large weights and encourages the model to use all input features in a more balanced way. This can be achieved through methods such as L1 and L2 regularization,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
    "\n",
    "sequences = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=100, #short answer\n",
    "    temperature=0.7, #randomness\n",
    "    top_k=50, #on how many top tokens to produce the search\n",
    "    top_p=0.95,#\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537e88d-3755-469d-acf5-e6197653a4db",
   "metadata": {},
   "source": [
    "# :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "725e340d-6d41-45ad-8804-0bb79b919cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a data scientist, how do you deal with class imbalance?\n",
      "\n",
      "## Answer (1)\n",
      "\n",
      "There is no one solution to imbalanced datasets. One of the most common methods is to oversample the minority class. This can be done using various techniques such as SMOTE (Synthetic Minority Over-sampling Technique). Another approach is to undersample the majority class.\n",
      "\n",
      "If the dataset is large enough, you could try to remove the majority class and retrain the model on the minority class only. This would be equivalent to undersampling the majority class.\n",
      "\n",
      "If you're using decision trees, you could try to use cost-sensitive learning.\n",
      "\n",
      "Comment: I was thinking of oversampling the minority class. Do you have any advice on how to do this?\n",
      "\n",
      "Comment: @Matthew You could try SMOTE or other oversampling techniques.\n",
      "\n",
      "Comment: SMOTE is a good option. I've used it before and it worked well.\n",
      "\n",
      "Comment: Thanks for the suggestion. I'll look into SMOTE.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As a data scientist, how do you deal with class imbalance?\"\n",
    "\n",
    "sequences = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=300, #short answer\n",
    "    temperature=0.7, #randomness\n",
    "    top_k=50, #on how many top tokens to produce the search\n",
    "    top_p=0.95,#\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976882f-e302-4f4b-9f79-9475972778f6",
   "metadata": {},
   "source": [
    "# Let's Switch to a Conversational Pipe:\n",
    "\n",
    "Mistral specifically expects an altenation of roles, so when dealing with this\n",
    "modality, you should alternate the roles, 1 or 3 sentencens. Here I use\n",
    "three because It allows me to condition it more stronly towards producing rhymes.\n",
    "IT is also easier to perform few shot learning by using an alternation of roles.\n",
    "You can give it examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d14fae43-ee71-4c3a-9011-b5f67b63cfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not the sun that rotates around the earth,\n",
      "It's the earth that rotates around the sun, with a girth.\n",
      "The sun is stationary, it doesn't spin or twirl,\n",
      "It's the earth that completes one full turn, with a whirl.\n",
      "So don't be mistaken, it's the earth that spins,\n",
      "And the sun is the center, where all the action begins.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"You are a friendly chatbot who always responds in rhymes\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure I am, pass me the ham!\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me, why it is not the sun that rotates aroudn the earth\"},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content']) #\"Epic Rap Battles of Data Science\" will soon be a thing, I promise..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f8a8f-1371-4336-97e2-0b160b5e86d6",
   "metadata": {},
   "source": [
    "# Let's use it to ask questions concerning a made up snippet now. \n",
    "\n",
    "Short examples, asking questions about predefined text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bedeead0-80bd-4339-aa39-51d7144e2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbr/anaconda3/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS. The service agent shows empathy in the conversation by acknowledging the customer's frustration and offering to help. The agent also provides clear and helpful information about the shipment's status and offers to follow up with the customer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation = \"\"\" Hello, Tammy speaking, you are talking with TML. \n",
    "Hello, Tammy, I would need to check a shipment I would need to receive this week. The traking number is\n",
    "999777999. Sure, madame, let's see what I find in the system. So here it says that it has been stopped at customs.\n",
    "Oh god. For how long is it going to stay in customs? It really depends on the problem, what I can do is to open a trace\n",
    "so that you can follow the development of the shipment. So if there is missing documentation, then we can directly\n",
    "contact you. Would that work for you madame? Yes, sure it would be really helpful. I am glad I could help madame,\n",
    "the process will therefore continue as follows. Someone from TML will contact you by email tomorrow, to report\n",
    "on the status of the shipment, you should receive the email on you TML account. Sure, thank you very much.\n",
    "Is there anything I can help you with, madame? No thank you very much, have a nice day. Have a nice day madame.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"You are an auditor of conversations between a service agent and a customer. You do not invent text. \n",
    "        You base your answers on the text only. \n",
    "        Given an aspect to evaluate, you report your answer as SUCCESS, FAILURE or Not Applicable. \n",
    "        You keep your answers short and synthetic.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I am ready to receive the text associated with the conversation. I will audit the following question: \" + question},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"This is the conversation to audit: \" + conversation},\n",
    "]\n",
    "\n",
    "result = pipe(messages, temperature = 0.1)\n",
    "print(result.messages[-1]['content']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5ac37-dddf-46cf-9769-c002e70a5664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "490ef180-53d7-4135-ad35-b95fc0a4fc7c",
   "metadata": {},
   "source": [
    "It decides for SUCCESS, despite the fact that the agent is not particularly emphatic here. \n",
    "I suppose, because Mistral has its issues defining what is Not Applicable in a context.\n",
    "It has to deal with the fact that it would have to decide on the basis of what is not happening. As a cognitive \n",
    "function it is called 'open world reasoning', so we would have to make it closed world. Let's try to get the three cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5ea44-3ed3-4039-8706-f4134392af69",
   "metadata": {},
   "source": [
    "# How to make it 'NOT APPLICABLE'. Very difficult. \n",
    "\n",
    "It is an ill posed question for Empathy. Think about it,\n",
    "a human answering a phone will answer in a human way, there is still some empathy in there.\n",
    "The simple fact that you are caring for the problem, show some empathy. In pure logical/language terms.\n",
    "We can say that, it requires higher cognitive skills and context than an LLM to see that case. \n",
    "This should make you understand the limit of automated reasoning. We can do it, as humans, because we have way more context and we can do \"meta-reasoning\". Despite all the hype in the industry around these models, that type of meta-reasoning is just very expensive to achieve, maybe very big LLMs can do SOMETHING around this. Smaller ones, very difficult. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6c80dab-cbd7-4f2b-8fa1-6f290bad42a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS. The service agent shows empathy by expressing regret and concern for the customer's inconvenience, and by offering to help the customer track the shipment and follow its development. The agent also shows empathy by acknowledging the customer's frustration and by offering to help the customer resolve the issue.\n"
     ]
    }
   ],
   "source": [
    "#adding rules\n",
    "\n",
    "question = \"Is the service agent showing empathy in the conversation?\" \n",
    "question = question + \"\"\" an additional rule to decide if there is a Failure, \n",
    "in this context, is that the agent should clearly be dismissive, close to frustrated. \n",
    "If the agent is simply using a neutral tone, Not Applicable can be selected. To select for a Success, the agent\n",
    "should express regret or apology openly with expressions such as 'I am sorry', 'I can imagine your disappointment', \n",
    "or participate in the emotion of the customer. Just solving the problem is not sufficient for Success in this auditing question.\"\"\"\n",
    "\n",
    "conversation_variation_na = \"\"\" Hello, Tammy speaking, you are talking with TML. \n",
    "Hello, Tammy, I would need to check a shipment I would need to receive this week. The traking number is\n",
    "999777999. Sure, madame, let's see what I find in the system. So here it says that it has been stopped at customs.\n",
    "Oh god. For how long is it going to stay in customs? What I can do is to open a trace\n",
    "so that you can follow the development of the shipment. So if there is missing documentation, then we can directly\n",
    "contact you. Would that work for you madame? Yes, sure it would be really helpful. I am glad I could help madame,\n",
    "the process will therefore continue as follows. Someone from TML will contact you by email tomorrow, to report\n",
    "on the status of the shipment, you should receive the email on you TML account. Sure, thank you very much.\n",
    "Is there anything I can help you with, madame? No thank you very much, have a nice day. Have a nice day madame.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"You are an auditor of conversations between a service agent and a customer. You do not invent text. \n",
    "        You base your answers on the text only. \n",
    "        Given an aspect to evaluate, you report your answer as SUCCESS, FAILURE or Not Applicable. \n",
    "        You keep your answers short and synthetic.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I am ready to receive the text associated with the conversation. I will audit the following question: \" + question},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"This is the conversation to audit: \" + conversation_variation_na},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117c6ec-6107-4328-b5ad-58f423358be1",
   "metadata": {},
   "source": [
    "# Success is easy to Spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9d51799-52f1-442a-8ca2-a3c3afbb0eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbr/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS. The service agent shows empathy in the conversation by expressing regret and apology, acknowledging the inconvenience and disappointment caused to the customer, and offering to open a trace to follow the development of the shipment.\n"
     ]
    }
   ],
   "source": [
    "#adding rules\n",
    "\n",
    "question = \"Is the service agent showing empathy in the conversation?\" \n",
    "question = question + \"\"\" an additional rule to decide if there is a Failure, \n",
    "in this context, is that the agent should clearly be dismissive, close to frustrated. \n",
    "If the agent is simply using a neutral tone, Not Applicable can be selected. To select for a Success, the agent\n",
    "should express regret or apology openly with expressions such as 'I am sorry', 'I can imagine your disappointment', \n",
    "or participate in the emotion of the customer. Just solving the problem is not sufficient for Success in this auditing question.\"\"\"\n",
    "\n",
    "conversation_variation_empathic = \"\"\" Hello, Tammy speaking, you are talking with TML. \n",
    "Hello, Tammy, I would need to check a shipment I would need to receive this week. The traking number is\n",
    "999777999. Sure, madame, let's see what I find in the system. So here it says that it has been stopped at customs.\n",
    "Oh god. For how long is it going to stay in customs? I am really sorry for this inconvient madame, I can image it is disappointing, \n",
    "but it really depends on the problem that the custom office is having. What I can do is to open a trace\n",
    "so that you can follow the development of the shipment. So if there is missing documentation, then we can directly\n",
    "contact you. Would that work for you madame? Yes, sure it would be really helpful. I am glad I could help madame,\n",
    "the process will therefore continue as follows. Someone from TML will contact you by email tomorrow, to report\n",
    "on the status of the shipment, you should receive the email on you TML account. Sure, thank you very much.\n",
    "Is there anything I can help you with, madame? No thank you very much, have a nice day. Have a nice day madame.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"You are an auditor of conversations between a service agent and a customer. You do not invent text. \n",
    "        You base your answers on the text only. \n",
    "        Given an aspect to evaluate, you report your answer as SUCCESS, FAILURE or Not Applicable. \n",
    "        You keep your answers short and synthetic.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I am ready to receive the text associated with the conversation. I will audit the following question: \" + question},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"This is the conversation to audit: \" + conversation_variation_empathic},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336ab2e-a979-4a1f-888b-15b17faf4b47",
   "metadata": {},
   "source": [
    "# Let's try a clearly dismissive Tone. Failure is easy to spot too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a5fc2d9-728a-4450-a51e-6064c425b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURE. The service agent is not showing empathy towards the customer. The agent dismisses the customer's concern and suggests that they contact the custom office, which is not helpful in this situation. The agent should have expressed regret or apology for the inconvenience caused to the customer and provided more information on how to contact the custom office.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Is the service agent showing empathy in the conversation?\" \n",
    "question = question + \"\"\" an additional rule to decide if there is a Failure, \n",
    "in this context, is that the agent should clearly be dismissive, close to frustrated. \n",
    "If the agent is simply using a neutral tone, Not Applicable can be selected. To select for a Success, the agent\n",
    "should express regret or apology openly with expressions such as 'I am sorry', 'I can imagine your disappointment', \n",
    "or participate in the emotion of the customer. Just solving the problem is not sufficient for Success in this auditing question.\"\"\"\n",
    "\n",
    "conversation_variation_dismissive = \"\"\" Hello, Tammy speaking, you are talking with TML. \n",
    "Hello, Tammy, I would need to check a shipment I would need to receive this week. The traking number is\n",
    "999777999. Sure, madame, let's see what I find in the system. So here it says that it has been stopped at customs.\n",
    "Oh god. For how long is it going to stay in customs? This is not an appropriate question for me, It is usually not a question I am able to answer, Eh, \n",
    "because it really depends on the problem that the custom office is having, you should ask them, not me, I do not take care of these things. \n",
    "Please, contact the custom office. Ok, have a nice day, bye. Have a nice day madame.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"You are an auditor of conversations between a service agent and a customer. You do not invent text. \n",
    "        You base your answers on the text only. \n",
    "        Given an aspect to evaluate, you report your answer as SUCCESS, FAILURE or Not Applicable. \n",
    "        You keep your answers short and synthetic.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I am ready to receive the text associated with the conversation. I will audit the following question: \" + question},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"This is the conversation to audit: \" + conversation_variation_dismissive},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb1848-eaa3-4e24-b127-d9a4d6d9f981",
   "metadata": {},
   "source": [
    "# Not Applicable, Round 2, Let's try few shot learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50d9857b-031d-44c5-94b3-e2ffbd2ad197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS. The service agent is showing empathy by acknowledging the customer's frustration and expressing regret for the inconvenience caused. The agent also offers a solution to the problem and provides the customer with a way to track the shipment. Additionally, the agent follows up with the customer and provides them with an email address to contact them for updates on the shipment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#adding rules\n",
    "\n",
    "question = \"Is the service agent showing empathy in the conversation?\" \n",
    "question = question + \"\"\" an additional rule to decide if there is a Failure, \n",
    "in this context, is that the agent should clearly be dismissive, close to frustrated. \n",
    "If the agent is simply using a neutral tone, Not Applicable can be selected. To select for a Success, the agent\n",
    "should express regret or apology openly with expressions such as 'I am sorry', 'I can imagine your disappointment', \n",
    "or participate in the emotion of the customer. Just solving the problem is not sufficient for Success in this auditing question.\"\"\"\n",
    "\n",
    "conversation_variation_na = \"\"\" Hello, Tammy speaking, you are talking with TML. \n",
    "Hello, Tammy, I would need to check a shipment I would need to receive this week. The traking number is\n",
    "999777999. Sure, madame. The shipment has been stopped at customs.\n",
    "Do you know For how long is it going to stay in customs? \n",
    "What I can do is to open a trace so that you can follow the development of the shipment. So if there is missing documentation, then we can directly\n",
    "contact you. The process will therefore continue as follows. Someone from TML will contact you by email tomorrow, to report\n",
    "on the status of the shipment, you should receive the email on you TML account. Sure, thank you very much.\n",
    "Is there anything I can help you with, madame? No thank you very much, have a nice day. Have a nice day madame.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"You are an auditor of conversations between a service agent and a customer. You do not invent text. \n",
    "        You base your answers on the text only. \n",
    "        Given an aspect to evaluate, you report your answer as SUCCESS, FAILURE or Not Applicable. \n",
    "        You keep your answers short and synthetic.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I am ready to receive the text associated with the conversation. I will audit the following question: \" + question},\n",
    "\n",
    "     {\"role\": \"user\", \"content\": \"\"\"an example sentence when empathy is not expressed \n",
    "     is 'I have a problem' and the answer to that is merely 'the solution of the problem is' or something similar. Offering a follow up, or further help is not a display of empathy either, it is merely part of the protocol followed by the service agent.\"\"\"},\n",
    "\n",
    "    {\"role\": \"assistant\", \"content\": \" so 'not applicable', concerning empathy is when no dismissive tone, but also no apology or regret are expressed. It is sort of a neutral, machine like, tone or way of expressing. \"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \" Correct. This is the conversation to audit: \" + conversation_variation_na},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content'])\n",
    "\n",
    "#Nope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b589f79-2352-4c7e-bdf3-ebcf2c17dbd3",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation, an example with HuggingFace and Mistral\n",
    "\n",
    "\"Retrieval-augmented generation\" refers to a hybrid approach that combines retrieval-based methods with generative methods in natural language processing (NLP). In this approach, a model leverages both the strengths of retrieval-based systems, which retrieve relevant information from a predefined set of responses or knowledge, and generative systems, which can create new, contextually relevant content.\n",
    "\n",
    "Here's an overview of how retrieval-augmented generation typically works:\n",
    "\n",
    "Retrieval-Based Component:\n",
    "\n",
    "The model first retrieves relevant information or responses from a predefined knowledge base or a set of candidate responses.\n",
    "This retrieval step is often based on similarity metrics, where the model compares the input or context with entries in the knowledge base to identify the most relevant information.\n",
    "Generative Component:\n",
    "\n",
    "Once relevant information is retrieved, a generative model, often based on techniques like transformers (e.g., GPT), uses this information as context to generate a coherent and contextually appropriate response.\n",
    "The generative component helps in producing responses that are not limited to pre-existing responses in the knowledge base, allowing for more creativity and flexibility in the generated content.\n",
    "By combining retrieval and generation, this approach aims to overcome limitations associated with purely generative or purely retrieval-based methods. Retrieval helps ensure that the generated responses are grounded in relevant information, while the generative component adds the ability to produce novel and diverse responses.\n",
    "\n",
    "This approach is particularly valuable in tasks such as dialogue systems, question answering, and content generation where a balance between leveraging existing knowledge and generating new, contextually relevant content is essential. It allows models to benefit from both the precision of retrieval-based methods and the creativity of generative methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14c1748a-7cf6-46a1-8194-ffc0fb0917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb\n",
    "#!pip install langchain\n",
    "#!pip install pypdf\n",
    "#!pip install scipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51df7131-23e2-406b-89d1-d5ef2e942063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in /home/sbr/anaconda3/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (4.7.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (1.26.18)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: numpy in /home/sbr/anaconda3/lib/python3.9/site-packages (from pinecone-client) (1.26.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/sbr/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sbr/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sbr/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sbr/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (2023.11.17)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install nltk --upgrade\n",
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83de39ff-07f2-47a2-9435-6e466c0b23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "loader = PyPDFLoader('./DeepLearningBigData.pdf')\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "# Split data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size = 4000,\n",
    "   chunk_overlap  = 20,\n",
    "   length_function = len,\n",
    "   add_start_index = True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbfdb03-501f-4f7d-baa9-58388e0a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [i.page_content for i in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a4d61c-ab80-4a6d-8406-4d7a486647f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path='./test_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62496566-47ab-4e53-8913-fbf0e2fa715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name='bigd4',\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e22ee0cd-a8c6-4366-a1b3-5d2fd0c5348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(documents=docs, ids=[f\"id{i}\" for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35371e7b-c142-4d59-9a43-40a7ae998af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be searching for results that are similar to this string\n",
    "#query_string = \"what is Deep Learning?\"\n",
    "\n",
    "# Perform the Chromadb query.\n",
    "\n",
    "def query_and_retrieve(qs, nr):\n",
    "    results = collection.query(\n",
    "        query_texts=[query_string],\n",
    "        n_results=2,\n",
    "    )\n",
    "\n",
    "    # Create a string from all of the results\n",
    "    results = '\\n'.join(results['documents'][0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7690490-7a16-4b36-8c7a-8717a51d942f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f013a908-6e17-4869-8b1d-8f39f3a0dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_mistral= PROMPT.format(context=results,question='What is Deep Learning?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec182bdb-d385-4e0a-ac87-3ef5bcc37118",
   "metadata": {},
   "source": [
    "# DEEP LEARNING out of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29fed1b5-6f51-4c74-94a1-91f23cb203e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers to automatically extract complex and abstract representations of data. It is motivated by the field of artificial intelligence and aims to emulate the hierarchical learning approach of the human brain. Deep Learning algorithms are designed to generalize in non-local and global ways, generating learning patterns and relationships beyond immediate neighbors in the data. They lead to abstract representations because more abstract representations are often constructed based on less abstract ones. Deep Learning algorithms are one promising avenue of research into the automated extraction of complex data representations at high levels of abstraction.\n"
     ]
    }
   ],
   "source": [
    "query_string = \"what is Deep Learning?\"\n",
    "\n",
    "pipe = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\" Instruction: You will be provided with questions and related data. \n",
    "        Your task is to find the answers to the questions using the given data. \n",
    "        If the data doesn't contain the answer to the question, then you must return 'Not enough information. \"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I see, the context is: \" + query_and_retrieve(query_string,3)},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Answer the following Question: \" +query_string},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content']) #\"Epic Rap Battles of Data Science\" will soon be a thing, I promise..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a07a62-061b-4f11-b003-027555b51a97",
   "metadata": {},
   "source": [
    "# Big Data out of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98177d51-fc7f-47c9-bbeb-71190d917ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data refers to the large and complex sets of data that traditional data processing methods cannot adequately handle. It is characterized by the four V's: Volume, Velocity, Variety, and Veracity. Big Data is often associated with the use of advanced data processing techniques, such as machine learning and data mining, to extract meaningful insights and patterns from the data. It is used in a variety of applications, including business intelligence, scientific research, and cybersecurity.\n"
     ]
    }
   ],
   "source": [
    "query_string = \"what is Big Data?\"\n",
    "\n",
    "pipe = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\" Instruction: You will be provided with questions and related data. \n",
    "        Your task is to find the answers to the questions using the given data. \n",
    "        If the data doesn't contain the answer to the question, then you must return 'Not enough information. \"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"I see, the context is: \" + query_and_retrieve(query_string,3)},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Answer the following Question: \" +query_string},\n",
    "]\n",
    "\n",
    "result = pipe(messages)\n",
    "print(result.messages[-1]['content']) #\"Epic Rap Battles of Data Science\" will soon be a thing, I promise..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
